\chapter{Построение решений задач}
\label{sec:Chapter3} \index{Chapter3}

После анализа литературы и уже существующих решений стало ясно, как реализовывать каждую из поставленных ранее задач. Далее будет рассказано о том, какие методы были выбраны для выполнения этих задач. Порядок, в котором они были объявлены в главе \ref{sec:Chapter1} сохранен.

\section{Признаки для датасета}

Обсудим признаки, которые были выбраны для обучения классификатора. В предыдущей главе \ref{sec:Chapter2} было сказано, что из всех признаков, указанных в \cite{HECKMAN2011363}, данная работа сфокусируется лишь на первых двух - характеристики ошибки и характеристики кода, опуская последние три - метрики репозитория с исходным кодом, метрики базы данных с багами, метрики динамического анализа. Далее будет описано, почему при составлении датасета был опущен каждый из перечисленных классов признаков, а также будет описано то, какие признаки все же были представлены в датасете в настоящей работе.

\subsection{Метрики репозитория с исходным кодом}
Как было обозначено в обзоре литературы \ref{sec:Chapter2}, для того, чтобы хоть сколько нибудь эффективно анализировать историю коммитов проекта, она должна быть строго организована, иначе поиск нужных мест в коде сводится к полному анализу всего проекта на каждом из этапов его разработки. В той же главе было объяснено, что в открытом доступе очень малое число проектов соблюдает подобные правила. Плюс, даже при их соблюдении, множественный анализ проекта очень дорог вычислительно и по времени. Плюс, для того, чтобы размечать false positives, необходимо иметь доступ к уже собранной статистике использования анализатора, которой не существует в открытом доступе. Таким образом, анализ истории коммитов и других метрик репозитория отпадает.

\subsection{Метрики базы данных с багами}
Как и в предыдущем пункте, применение данных метрик ограничивается их отсутствием в открытом доступе.

\subsection{Метрики динамического анализа}
Для проведения динамического анализа требуется полная сборка всего проекта, что добавляет сложности к и без того затратному процессу анализа. Также, основным предметом изучения данной работы является именно статический анализ, поэтому рассмотрение метрик динамического анализа было решено оставить как предмет для будущих исследований.

\subsection{Характеристики ошибки}
\label{sec:Err-to-CWE} \index{Err-to-CWE}
Для того, чтобы удовлетворить требованию языковой независимости, все сообщения анализаторов требуется приводить к общему виду. Таким общим видом было выбрано представление Common Weakness Enumeration (CWE)\cite{CWE-doc}. Таким образом, добавление нового статического анализатора к списку поддерживаемых сводится к добавлению интерфейса, транслирующего код ошибки анализатора в код CWE. К счастью, для большинства распространенных статических анализаторов существуют таблицы соответствия или настройки, позволяющие выводить код ошибки сразу в формате CWE.

\subsection{Характеристики кода}
В проанализированной литературе основными признаками, по которым производилось обучение, являлись метрики кода. Чаще всего речь шла о метриках таких как вложенность, цикломатическая сложность, количество путей через выделенный фрагмент кода, etc. В данной работе метрики кода считались только для функции, в которой анализатор обнаружил уязвимость. Замеры производились при помощи утилиты ccsm\cite{CCSM}, в которую были дополнительно внесены изменения. Реализована данная утилита как инструмент в инфраструктуре Clang\cite{Clang}. Этот инструмент вызывает парсер Clang для указанного файла, и работает с полученным частичным промежуточным представлением. Т.к. полная компиляция проекта не требуется (парсинг производится лишь на выбранном файле), то извлечение этих метрик не сказывается на общем времени сбора датасета. Метрики кода, использованные для обучения моделей в данной работе можно разделить на следующие группы:

\begin{enumerate}
    \item Количество ключевых слов, контролирующих поток исполнения программы (for, if, else, etc.)
    \item Различные способы подсчета цикломатической сложности
    \item Количество различных обращений к памяти (разименование указателя, обращение к полю, обращение к полю по указателю, etc.)
    \item Характеристики самой функции (количество путей через нее, вложенность)
\end{enumerate}

\subsection{Token-based представление кода}
\label{sec:Tokens} \index{Tokens}
Описанные выше метрики характеризуют код лишь косвенно, не давая представления о его структуре. Таким образом теряется способность классификатора распознавать шаблоны, которые приводят к false positive. Чтобы бороться с этой проблемой был предложен подход на основе токенов. Токены были выбраны, т.к. анализ AST является затруднительным и более затратным. Токены, с другой стороны, можно получить еще на этапе лексического анализа. Вывод токенов был также реализован в качестве инструмента для Clang. Чтобы увеличить шансы на распознавание шаблонов false positive, представление кода в виде токенов было предварительно обработано: были оставлены только ключевые слова, влияющие на поток исполнения, операторы и идентификаторы.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{flow.png}
    \caption{Шаги получения отфильтрованной последовательности токенов}
\end{figure}

После предварительной обработки, из получившейся последовательности токенов выбирается подпоследовательность фиксированной длины, такая, что токен, на который указывает сообщение об ошибке находится ровно посередине подпоследовательности. Получившаяся подпоследовательность фиксированной длины называется окном и контролируется гиперпараметром WINDOW\_SIZE, отвечающим за ширину этого окна. Соответственно, в датасете появляются признаки token[0], token[1], token[2], ... token[WINDOW\_SIZE - 1]. Предположение данной работы состоит в том, что последовательности отфильтрованных токенов наряду с метаданными о коде должно хватить для распознавания шаблонов false positive.
\\
\\
Резюмируя, датасет, используемый для обучения моделей в данной работе, имеет следующий вид:

\begin{table}[H]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{|c|c|c|c|c|c||c|}
            \hline
            код CWE & token[0] & token[1] & ... & token[WINDOW\_SIZE - 1] & метрики кода & метка класса \\ \hline
        \end{tabular}
    }
    \caption{Общий вид элемента датасета}
\end{table}

\section{Метод генерации и разметки датасета}



\section{Сбор данных}

\section{Обучение классификатора}

\subsection{Decision tree}
\subsection{Gradient boosting}

Рассмотрим пример кода Juliet Test Suite:

\begin{verbatim}
    void CWE415_Double_Free__malloc_free_char_08_bad()
    {
        char * data;
        /* Initialize data */
        data = NULL;
        if(staticReturnsTrue())
        {
            data = (char *)malloc(100*sizeof(char));
            if (data == NULL) {exit(-1);}
            /* POTENTIAL FLAW: Free data in the source - the bad sink frees data as well */
            free(data);
        }
        if(staticReturnsTrue())
        {
            /* POTENTIAL FLAW: Possibly freeing memory twice */
            free(data);
        }
    }
\end{verbatim}

The Juliet Test Suite contains two kinds of metadata that
are relevant for determining the validity of alerts:
a manifest file: This is an XML file that provides precise
flaw information including line number, CWE, and
filepath.
function names: Documentation for the test suite says
that if the function name includes the string GOOD then
the particular CWE does not occur in it, but if it includes
the string BAD then the CWE does occur in the function.
We gathered information about filepath and line numbers
covered by each function name that contains GOOD or
BAD, as well as the CWE indicated (usually by filename).
Note that both the manifest file and the function names
provide only CWE-specific flaw information. In general, a line
of code marked BAD for one code flaw type could be flawless
with respect to all other code flaw types. Thus, we can use the
metadata flaw information to determine the validity of an alert
only when we can establish that the alert’s checkerID is for a
flaw of the same type as a flaw referenced in the metadata. The
test suite metadata does not identify every CWE weakness in
the Juliet code nor all locations of the CWE weaknesses, so
an alert that doesn’t map to the test suite metadata cannot be
automatically labeled using the metadata. In other words, if
an alert’s CWE doesn’t match the test suite metadata’s CWE,
the metadata can’t be used to label the alert true or false.
Publicly-available mappings between checkerIDs and
CWEs are available for many of the FFSA tools that we tested.
We fused alerts from this set of tools, producing a set of fused
alerts with known CWE designations. We then determined
verdicts (i.e. classifier ground truth labels) for each fused alert
as follows:
If the manifest includes a record of a flaw for the
same filepath, line number, and CWE as the alert, then
set verdict=True, indicating that the alert is a true
positive.
If the defect alert matches any line within a function
name with GOOD and the alert’s CWE matches the CWE
associated with the function, then set verdict=False,
indicating a that the alert is a false positive.


\newpage